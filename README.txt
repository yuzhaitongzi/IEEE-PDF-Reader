程序创新点：
对比普通的自然语言处理英语单词词频，无法做到中文（如jieba)对词组进行组合进行的词频分析
此程序实现了英语单词进行词性组合
main中：
# 规定分词块的语法
grammar = r"""
  NP: {<DT|PP\$>?<JJ>*<NN>}   # 匹配形容词+名词/动名词/过去分词
      {<DT>+<VBG|VBN>}           #匹配形容词+动名词/过去分词
      {<DT>?<JJ>*<NN.*>}      #匹配冠词+形容词+名词  .*表示可以出现一个或者多个
      {<NNP|NNPS>+}                # 匹配连续的专有名词或者复数形式
      {<FW.*>}                  #匹配外来语，外来词，外文原词
      {<PRP>}                 # 匹配人称代词
"""
你也可以后续自行添加你需要的词组
这样的好处是可以在巨量词汇中提取关键词时出现的不是一个泛泛而谈的概念英语单词，便于对文章进行热词提取
出于一篇论文的数据量有限，更广阔的用途可以用来对标科研风向，例如获取新一轮IEEE论文中出现最高的词组



将你需要提取关键词的PDF放入文件中，并在main函数里更改pdf的路径
通过此代码实现将IEEE论文的pdf转化为要提取的全部文本（去除了页眉页脚，参考文献等，只保留了主题内容）在body.txt中，并没有进行分句处理。
转化为的主要内容以及分句版本存放在output.txt中
其中output.txt中出现的  ” , : , : , : , : }, : , : , : IndirectObject(52, 0, 2350581939024), : 0, : }“
这一长串文字为pdf分页时的信息，尝试正则表达式替换，replace几个字都没有办法去除，只能留下来当一个换页标记
分句因为是判断有特殊符号才不分的，但出现了人名 比如 Dc.White 还是会出现分句现象，不过这类情况出现较少，不影响阅读。
此时得到的keywords.txt是一个存在了很多没必要的词汇（例如 a ,I,t,一些单位等），你可以在stopwords.txt里添加不想要的词语
在运行一次stopwords.py，即可得到result,是删除了你决定的不重要词后的词频数据